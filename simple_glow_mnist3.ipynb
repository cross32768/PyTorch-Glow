{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision verseion: 0.2.1\n",
      "Is GPU avaibale: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision verseion:', torchvision.__version__)\n",
    "print('Is GPU avaibale:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings (バッチサイズとデバイス)\n",
    "batchsize = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of training data: 60000\n",
      "the number of validation data: 10000\n"
     ]
    }
   ],
   "source": [
    "# データセットの準備\n",
    "# Tensorにしつつ、 (-1 ~ 1)の範囲に正規化\n",
    "\n",
    "#def preprocess(tensor):\n",
    "#    return tensor - 0.5\n",
    "\n",
    "tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# データセットをロード(今回はMNIST)\n",
    "# 本当はtraining data, validation data, test dataに分けるべきだが、今回は簡便のため2つに分ける.\n",
    "mnist_train = datasets.MNIST(root = '../../data/MNIST',\n",
    "                                 train = True,\n",
    "                                 transform = tf,\n",
    "                                 download = False)\n",
    "mnist_validation = datasets.MNIST(root = '../../data/MNIST',\n",
    "                                      train = False,\n",
    "                                      transform = tf)\n",
    "\n",
    "# データローダーを作成\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size = batchsize, shuffle = True)\n",
    "mnist_validation_loader = DataLoader(mnist_validation, batch_size = batchsize, shuffle = False)\n",
    "\n",
    "print('the number of training data:', len(mnist_train))\n",
    "print('the number of validation data:', len(mnist_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actnormの実装\n",
    "class ActNorm2d(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ActNorm2d, self).__init__()\n",
    "        size = [1, num_features, 1, 1]\n",
    "        self.register_parameter('bias', nn.Parameter(torch.zeros(*size)))\n",
    "        self.register_parameter('log_s', nn.Parameter(torch.zeros(*size)))\n",
    "        self.inited = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.inited:\n",
    "            self.initialize_parameters(x)\n",
    "        \n",
    "        z = torch.exp(self.log_s) * (x + self.bias)\n",
    "        log_det_jacobian = self.calculate_log_det_jacobian(x)\n",
    "        return z, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        x = z * torch.exp(-self.log_s) - self.bias\n",
    "        return x\n",
    "\n",
    "    def calculate_log_det_jacobian(self, x):\n",
    "        h, w = x.size(2), x.size(3)\n",
    "        return h * w * torch.sum(self.log_s)\n",
    "    \n",
    "    def initialize_parameters(self, first_minibatch_x):\n",
    "        with torch.no_grad():\n",
    "            bias = -1.0 * self.multidim_mean(first_minibatch_x.clone(), dims=[0, 2, 3])\n",
    "            var_s = self.multidim_mean((first_minibatch_x.clone() + bias) ** 2, dims=[0, 2, 3])\n",
    "            log_s = torch.log(1 / (torch.sqrt(var_s) + 1e-6))\n",
    "        \n",
    "            self.bias.data.copy_(bias.data)\n",
    "            self.log_s.data.copy_(log_s.data)\n",
    "        \n",
    "            self.inited = True\n",
    "            \n",
    "    def multidim_mean(self, tensor, dims):\n",
    "        dims = sorted(dims)\n",
    "        for d in dims:\n",
    "            tensor = tensor.mean(dim=d, keepdim=True)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invertible 1x1 convolutionの実装\n",
    "class Invertible1x1Conv2d(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Invertible1x1Conv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(num_features, num_features, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        W = torch.qr(torch.FloatTensor(num_features, num_features).normal_())[0]\n",
    "        \n",
    "        if torch.det(W) < 0:\n",
    "            W[:,0] = -W[:,0]\n",
    "\n",
    "        self.conv.weight.data = W.view(num_features, num_features, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.conv(x)\n",
    "        log_det_jacobian = self.calculate_log_det_jacobian(x)\n",
    "        return z, log_det_jacobian\n",
    "        \n",
    "    def inverse(self, z, train_finished=False):\n",
    "        if train_finished:\n",
    "            if not hasattr(self, 'W_inverse'):\n",
    "                W = self.conv.weight.squeeze()\n",
    "                W_inverse = W.inverse()\n",
    "                self.W_inverse = W_inverse.view(*W_inverse.size(), 1, 1)\n",
    "            x = F.conv2d(z, self.W_inverse, bias=None, stride=1, padding=0)\n",
    "        else:\n",
    "            W = self.conv.weight.squeeze()\n",
    "            W_inverse = W.inverse().view(*W.size(), 1, 1)\n",
    "            x = F.conv2d(z, W_inverse, bias=None, stride=1, padding=0)\n",
    "        return x\n",
    "        \n",
    "    def calculate_log_det_jacobian(self, x):\n",
    "        W = self.conv.weight.squeeze()\n",
    "        h, w = x.size(2), x.size(3)\n",
    "        return h * w * torch.logdet(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupling layerで使われるCNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, affine=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.affine = affine\n",
    "        if affine:\n",
    "            n_out = n_in*2\n",
    "        else:\n",
    "            n_out = n_in\n",
    "            \n",
    "        self.cv1 = nn.Conv2d(n_in, n_hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.ac1 = ActNorm2d(n_hidden)\n",
    "        self.cv2 = nn.Conv2d(n_hidden, n_hidden, kernel_size=1, stride=1, padding=0)\n",
    "        self.ac2 = ActNorm2d(n_hidden)\n",
    "        self.cv3 = nn.Conv2d(n_hidden, n_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, CNN_input):\n",
    "        out = F.relu(self.ac1(self.cv1(CNN_input))[0])\n",
    "        out = F.relu(self.ac2(self.cv2(out))[0])\n",
    "        # out = F.relu(self.cv1(CNN_input))\n",
    "        # out = F.relu(self.cv2(out))\n",
    "        if self.affine:\n",
    "            out = self.cv3(out)\n",
    "            n_half = int(out.size(1) / 2)\n",
    "            log_s = torch.tanh(out[:,:n_half,:,:])\n",
    "            bias = out[:,n_half:,:,:]\n",
    "            return [log_s, bias]\n",
    "        else:\n",
    "            bias = self.cv3(out)\n",
    "            return bias\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.cv1.weight.data.normal_(0, 0.05)\n",
    "        self.cv1.bias.data.zero_()\n",
    "        self.cv2.weight.data.normal_(0, 0.05)\n",
    "        self.cv2.bias.data.zero_()\n",
    "        self.cv3.weight.data.zero_()\n",
    "        self.cv3.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupling layerの実装\n",
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, num_features, n_hidden, affine=True):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "        \n",
    "        assert num_features % 2 == 0\n",
    "        self.n_half = int(num_features / 2)\n",
    "        self.affine = affine\n",
    "        \n",
    "        self.CNN = CNN(self.n_half, n_hidden, affine)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_a = x[:,:self.n_half,:,:]\n",
    "        x_b = x[:,self.n_half:,:,:]\n",
    "        \n",
    "        CNN_output = self.CNN(x_a)\n",
    "        \n",
    "        if self.affine:\n",
    "            log_s = CNN_output[0]\n",
    "            bias = CNN_output[1]\n",
    "            z_b = torch.exp(log_s) * (x_b + bias)\n",
    "        else:\n",
    "            log_s = None\n",
    "            z_b = x_b + CNN_output\n",
    "            \n",
    "        z = torch.cat([x_a, z_b], dim=1)\n",
    "        log_det_jacobian = self.calculate_log_det_jacobian(log_s)\n",
    "        return z, log_det_jacobian\n",
    "        \n",
    "    def inverse(self, z):\n",
    "        z_a = z[:,:self.n_half,:,:]\n",
    "        z_b = z[:,self.n_half:,:,:]\n",
    "        \n",
    "        CNN_output = self.CNN(z_a)\n",
    "        \n",
    "        if self.affine:\n",
    "            log_s = CNN_output[0]\n",
    "            bias = CNN_output[1]\n",
    "            x_b = z_b * torch.exp(-log_s) - bias\n",
    "        else:\n",
    "            x_b = z_b - CNN_output\n",
    "            \n",
    "        x = torch.cat([z_a, x_b], dim=1)\n",
    "        return x\n",
    "        \n",
    "    def calculate_log_det_jacobian(self, log_s):\n",
    "        if self.affine:\n",
    "            return torch.sum(log_s) / log_s.size(0)\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上３つをまとめたFlow\n",
    "class StepofFlow(nn.Module):\n",
    "    def __init__(self, num_features, n_hidden, affine=True):\n",
    "        super(StepofFlow, self).__init__()\n",
    "        self.actnorm = ActNorm2d(num_features)\n",
    "        self.invertible1x1conv = Invertible1x1Conv2d(num_features)\n",
    "        self.couplinglayer = CouplingLayer(num_features, n_hidden, affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, ldj_actnorm  = self.actnorm(x)\n",
    "        # print('after_actnorm', torch.mean(torch.abs(x)))\n",
    "        # print('act', ldj_actnorm)\n",
    "        x, ldj_1x1conv  = self.invertible1x1conv(x)\n",
    "        # print('after_1x1conv', torch.mean(torch.abs(x)))\n",
    "        # print('1x1conv', ldj_1x1conv)\n",
    "        z, ldj_coupling = self.couplinglayer(x)\n",
    "        # print('after_coupling', torch.mean(torch.abs(z)))\n",
    "        # print('coupling', ldj_coupling)\n",
    "        log_det_jacobian = ldj_actnorm + ldj_1x1conv + ldj_coupling\n",
    "        return z, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, z, train_finished=False):\n",
    "        z = self.couplinglayer.inverse(z)\n",
    "        z = self.invertible1x1conv.inverse(z, train_finished)\n",
    "        x = self.actnorm.inverse(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glow本体\n",
    "class Glow(nn.Module):\n",
    "    def __init__(self, L, K, num_input_features, n_hidden_list, affine=True):\n",
    "        super(Glow, self).__init__()\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        \n",
    "        num_features = num_input_features\n",
    "        assert len(n_hidden_list) == L*K\n",
    "        \n",
    "        self.flow = torch.nn.ModuleList()\n",
    "        for l in range(L):\n",
    "            # squeeze\n",
    "            num_features *= 4\n",
    "            for k in range(K):\n",
    "                # step of flow\n",
    "                self.flow.append(StepofFlow(num_features, int(n_hidden_list[l*K + k]), affine))\n",
    "            # split\n",
    "            num_features = num_features // 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = []\n",
    "        log_det_jacobian = 0\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            # squeeze\n",
    "            x = self.squeeze(x)\n",
    "            for k in range(self.K):\n",
    "                # step of flow\n",
    "                x, ldj = self.flow[l*self.K + k](x)\n",
    "                log_det_jacobian += ldj\n",
    "            # split\n",
    "            if l == self.L-1:\n",
    "                z.append(x.view(x.size(0), -1))\n",
    "            else:\n",
    "                z.append(x[:,:x.size(1)//2,:,:].view(x.size(0), -1))\n",
    "                x = x[:,x.size(1)//2:,:,:]\n",
    "        \n",
    "        z = torch.cat(z, dim=1)\n",
    "        if not hasattr(self, 'Z'):\n",
    "            batchsize, Z_dim = z.size()\n",
    "            self.Z = MultivariateNormal(torch.zeros(Z_dim).to(device), torch.eye(Z_dim).to(device))\n",
    "            self.last_z_shape = x.size()[1:]\n",
    "            \n",
    "        return z, log_det_jacobian\n",
    "        \n",
    "    def inverse(self, z, train_finished=False):\n",
    "        x_dim = self.last_z_shape[0] * self.last_z_shape[1] * self.last_z_shape[2]\n",
    "        for l in reversed(range(self.L)):\n",
    "            if l == self.L-1:\n",
    "                x = z[:,-x_dim:].view(-1, *self.last_z_shape)\n",
    "            else:\n",
    "                z_in = z[:,-x_dim*2:-x_dim]\n",
    "                x = torch.cat([z_in.view(*x.size()), x], dim = 1)\n",
    "                x_dim = x_dim*2\n",
    "                \n",
    "            for k in reversed(range(self.K)):\n",
    "                x = self.flow[l*self.K + k].inverse(x, train_finished)\n",
    "                \n",
    "            x = self.unsqueeze(x)\n",
    "        return x\n",
    "                \n",
    "    def squeeze(self, x, factor=2):\n",
    "        batchsize, channels, height, width = x.size()\n",
    "        assert height % factor == 0\n",
    "        assert width % factor == 0\n",
    "        z = x.view(batchsize, channels, height // factor, factor, width // factor, factor)\n",
    "        z = z.permute(0, 1, 3, 5, 2, 4)\n",
    "        z = z.contiguous().view(batchsize, channels * factor**2, height // factor, width // factor)\n",
    "        return z\n",
    "    \n",
    "    def unsqueeze(self, z, factor=2):\n",
    "        batchsize, channels, height, width = z.size()\n",
    "        x = z.view(batchsize, channels // (factor**2), factor, factor, height, width)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3)\n",
    "        x = x.contiguous().view(batchsize, channels // (factor**2), height * factor, width * factor)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 882496\n"
     ]
    }
   ],
   "source": [
    "net = Glow(L=2, K=16, num_input_features=1, n_hidden_list=np.ones(2*16)*128, affine=True)\n",
    "net = net.to(device)\n",
    "\n",
    "# warm_up_epochs = 10\n",
    "learning_rate = 0.001 # * (10 ** (-warm_up_epochs))\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "n_epochs = 100\n",
    "save_image_interval = 1\n",
    "n_save_image = 25\n",
    "save_dir = '../../data/glow_MNIST/'\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters:', num_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    for batch_index, sample_x in enumerate(train_loader):\n",
    "        sample_x = sample_x[0].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict_z, log_det_jacobian = net(sample_x)\n",
    "        log_p_z = net.Z.log_prob(predict_z)\n",
    "        log_p_z_mean = torch.mean(log_p_z) / predict_z.size(1)\n",
    "        log_det_jacobian = log_det_jacobian / predict_z.size(1)\n",
    "        loss = (log_p_z_mean - log_det_jacobian)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        print('batch_index[%3d/%3d] log_p_z_mean:%1.5f log_det_jacobian:%1.5f' \\\n",
    "            % (batch_index+1, len(train_loader), log_p_z_mean.item(), log_det_jacobian.item()))\n",
    "        \n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(validation_loader, epoch):\n",
    "    net.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sample_x, _ in validation_loader:\n",
    "            sample_x = sample_x.to(device)\n",
    "            \n",
    "            predict_z, log_det_jacobian = net(sample_x)\n",
    "            log_p_z = net.Z.log_prob(predict_z)\n",
    "            log_p_z_mean = torch.mean(log_p_z) / predict_z.size(1)\n",
    "            log_det_jacobian = log_det_jacobian / predict_z.size(1)\n",
    "            loss = -(log_p_z_mean + log_det_jacobian)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if epoch % save_image_interval == 0:\n",
    "            sample_z = net.Z.sample((n_save_image,))\n",
    "            predict_x = net.inverse(sample_z)\n",
    "            print(torch.mean(predict_x))\n",
    "            save_image(predict_x.data.cpu(), '{}/epoch_{}.png'.format(save_dir, epoch), nrow=5, normalize=True)\n",
    "            \n",
    "    return running_loss / len(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index[  1/469] log_p_z_mean:-1.41766 log_det_jacobian:1.01319\n",
      "batch_index[  2/469] log_p_z_mean:-0.94205 log_det_jacobian:-1.64657\n",
      "batch_index[  3/469] log_p_z_mean:-0.93810 log_det_jacobian:-4.29338\n",
      "batch_index[  4/469] log_p_z_mean:-0.95632 log_det_jacobian:-7.44805\n",
      "batch_index[  5/469] log_p_z_mean:-1.74472 log_det_jacobian:-9.99125\n",
      "batch_index[  6/469] log_p_z_mean:-1.32199 log_det_jacobian:-10.97683\n",
      "batch_index[  7/469] log_p_z_mean:-2.46990 log_det_jacobian:-11.22617\n",
      "batch_index[  8/469] log_p_z_mean:-0.96171 log_det_jacobian:-11.17433\n",
      "batch_index[  9/469] log_p_z_mean:-0.96874 log_det_jacobian:-11.15997\n",
      "batch_index[ 10/469] log_p_z_mean:-0.99012 log_det_jacobian:-11.22556\n",
      "batch_index[ 11/469] log_p_z_mean:-1.01690 log_det_jacobian:-11.32691\n",
      "batch_index[ 12/469] log_p_z_mean:-1.03288 log_det_jacobian:-11.43548\n",
      "batch_index[ 13/469] log_p_z_mean:-1.02215 log_det_jacobian:-11.53787\n",
      "batch_index[ 14/469] log_p_z_mean:-0.99108 log_det_jacobian:-11.61552\n",
      "batch_index[ 15/469] log_p_z_mean:-0.95598 log_det_jacobian:-11.67116\n",
      "batch_index[ 16/469] log_p_z_mean:-0.93811 log_det_jacobian:-11.72606\n",
      "batch_index[ 17/469] log_p_z_mean:-0.94414 log_det_jacobian:-11.78935\n",
      "batch_index[ 18/469] log_p_z_mean:-0.99458 log_det_jacobian:-11.84815\n",
      "batch_index[ 19/469] log_p_z_mean:-1.14096 log_det_jacobian:-11.89895\n",
      "batch_index[ 20/469] log_p_z_mean:-0.92660 log_det_jacobian:-11.93647\n",
      "batch_index[ 21/469] log_p_z_mean:-0.93545 log_det_jacobian:-11.98587\n",
      "batch_index[ 22/469] log_p_z_mean:-0.93869 log_det_jacobian:-12.03560\n",
      "batch_index[ 23/469] log_p_z_mean:-0.93496 log_det_jacobian:-12.08530\n",
      "batch_index[ 24/469] log_p_z_mean:-0.93079 log_det_jacobian:-12.13507\n",
      "batch_index[ 25/469] log_p_z_mean:-0.92837 log_det_jacobian:-12.18465\n",
      "batch_index[ 26/469] log_p_z_mean:-0.92709 log_det_jacobian:-12.23374\n",
      "batch_index[ 27/469] log_p_z_mean:-0.92587 log_det_jacobian:-12.28227\n",
      "batch_index[ 28/469] log_p_z_mean:-0.92534 log_det_jacobian:-12.33065\n",
      "batch_index[ 29/469] log_p_z_mean:-0.92622 log_det_jacobian:-12.37946\n",
      "batch_index[ 30/469] log_p_z_mean:-0.92793 log_det_jacobian:-12.42874\n",
      "batch_index[ 31/469] log_p_z_mean:-0.92890 log_det_jacobian:-12.47878\n",
      "batch_index[ 32/469] log_p_z_mean:-0.92830 log_det_jacobian:-12.52940\n",
      "batch_index[ 33/469] log_p_z_mean:-0.92653 log_det_jacobian:-12.58043\n",
      "batch_index[ 34/469] log_p_z_mean:-0.92452 log_det_jacobian:-12.63186\n",
      "batch_index[ 35/469] log_p_z_mean:-0.92304 log_det_jacobian:-12.68340\n",
      "batch_index[ 36/469] log_p_z_mean:-0.92222 log_det_jacobian:-12.73511\n",
      "batch_index[ 37/469] log_p_z_mean:-0.92175 log_det_jacobian:-12.78668\n",
      "batch_index[ 38/469] log_p_z_mean:-0.92130 log_det_jacobian:-12.83861\n",
      "batch_index[ 39/469] log_p_z_mean:-0.92086 log_det_jacobian:-12.89066\n",
      "batch_index[ 40/469] log_p_z_mean:-0.92067 log_det_jacobian:-12.94281\n",
      "batch_index[ 41/469] log_p_z_mean:-0.92077 log_det_jacobian:-12.99524\n",
      "batch_index[ 42/469] log_p_z_mean:-0.92093 log_det_jacobian:-13.04777\n",
      "batch_index[ 43/469] log_p_z_mean:-0.92098 log_det_jacobian:-13.10049\n",
      "batch_index[ 44/469] log_p_z_mean:-0.92093 log_det_jacobian:-13.15341\n",
      "batch_index[ 45/469] log_p_z_mean:-0.92080 log_det_jacobian:-13.20652\n",
      "batch_index[ 46/469] log_p_z_mean:-0.92060 log_det_jacobian:-13.25997\n",
      "batch_index[ 47/469] log_p_z_mean:-0.92032 log_det_jacobian:-13.31355\n",
      "batch_index[ 48/469] log_p_z_mean:-0.91999 log_det_jacobian:-13.36744\n",
      "batch_index[ 49/469] log_p_z_mean:-0.91972 log_det_jacobian:-13.42154\n",
      "batch_index[ 50/469] log_p_z_mean:-0.91961 log_det_jacobian:-13.47595\n",
      "batch_index[ 51/469] log_p_z_mean:-0.91969 log_det_jacobian:-13.53054\n",
      "batch_index[ 52/469] log_p_z_mean:-0.91984 log_det_jacobian:-13.58532\n",
      "batch_index[ 53/469] log_p_z_mean:-0.91994 log_det_jacobian:-13.64029\n",
      "batch_index[ 54/469] log_p_z_mean:-0.91999 log_det_jacobian:-13.69557\n",
      "batch_index[ 55/469] log_p_z_mean:-0.91997 log_det_jacobian:-13.75109\n",
      "batch_index[ 56/469] log_p_z_mean:-0.91991 log_det_jacobian:-13.80683\n",
      "batch_index[ 57/469] log_p_z_mean:-0.91978 log_det_jacobian:-13.86272\n",
      "batch_index[ 58/469] log_p_z_mean:-0.91959 log_det_jacobian:-13.91886\n",
      "batch_index[ 59/469] log_p_z_mean:-0.91943 log_det_jacobian:-13.97529\n",
      "batch_index[ 60/469] log_p_z_mean:-0.91934 log_det_jacobian:-14.03189\n",
      "batch_index[ 61/469] log_p_z_mean:-0.91935 log_det_jacobian:-14.08875\n",
      "batch_index[ 62/469] log_p_z_mean:-0.91938 log_det_jacobian:-14.14579\n",
      "batch_index[ 63/469] log_p_z_mean:-0.91938 log_det_jacobian:-14.20309\n",
      "batch_index[ 64/469] log_p_z_mean:-0.91937 log_det_jacobian:-14.26061\n",
      "batch_index[ 65/469] log_p_z_mean:-0.91934 log_det_jacobian:-14.31834\n",
      "batch_index[ 66/469] log_p_z_mean:-0.91931 log_det_jacobian:-14.37632\n",
      "batch_index[ 67/469] log_p_z_mean:-0.91925 log_det_jacobian:-14.43452\n",
      "batch_index[ 68/469] log_p_z_mean:-0.91919 log_det_jacobian:-14.49293\n",
      "batch_index[ 69/469] log_p_z_mean:-0.91914 log_det_jacobian:-14.55156\n",
      "batch_index[ 70/469] log_p_z_mean:-0.91914 log_det_jacobian:-14.61043\n",
      "batch_index[ 71/469] log_p_z_mean:-0.91918 log_det_jacobian:-14.66948\n",
      "batch_index[ 72/469] log_p_z_mean:-0.91921 log_det_jacobian:-14.72881\n",
      "batch_index[ 73/469] log_p_z_mean:-0.91920 log_det_jacobian:-14.78830\n",
      "batch_index[ 74/469] log_p_z_mean:-0.91916 log_det_jacobian:-14.84805\n",
      "batch_index[ 75/469] log_p_z_mean:-0.91911 log_det_jacobian:-14.90801\n",
      "batch_index[ 76/469] log_p_z_mean:-0.91908 log_det_jacobian:-14.96815\n",
      "batch_index[ 77/469] log_p_z_mean:-0.91908 log_det_jacobian:-15.02857\n",
      "batch_index[ 78/469] log_p_z_mean:-0.91909 log_det_jacobian:-15.08916\n",
      "batch_index[ 79/469] log_p_z_mean:-0.91909 log_det_jacobian:-15.14998\n",
      "batch_index[ 80/469] log_p_z_mean:-0.91909 log_det_jacobian:-15.21108\n",
      "batch_index[ 81/469] log_p_z_mean:-0.91907 log_det_jacobian:-15.27236\n",
      "batch_index[ 82/469] log_p_z_mean:-0.91906 log_det_jacobian:-15.33386\n",
      "batch_index[ 83/469] log_p_z_mean:-0.91905 log_det_jacobian:-15.39555\n",
      "batch_index[ 84/469] log_p_z_mean:-0.91904 log_det_jacobian:-15.45749\n",
      "batch_index[ 85/469] log_p_z_mean:-0.91905 log_det_jacobian:-15.51963\n",
      "batch_index[ 86/469] log_p_z_mean:-0.91905 log_det_jacobian:-15.58201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-31bd83c0fdb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_validation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-f1c66fc17df3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlog_det_jacobian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_det_jacobian\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpredict_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p_z_mean\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_det_jacobian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_nll_list = []\n",
    "validation_nll_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_nll = train(mnist_train_loader)\n",
    "    validation_nll = validation(mnist_validation_loader, epoch)\n",
    "    \n",
    "    train_nll_list.append(train_nll)\n",
    "    validation_nll_list.append(validation_nll)\n",
    "    \n",
    "    # if epoch < warm_up_epochs:\n",
    "    #   optimizer.param_groups[0]['lr'] *= 10\n",
    "    \n",
    "    # print(optimizer.param_groups[0]['lr'])\n",
    "    print('epoch[%2d/%2d] train_nll:%1.4f validation_nll:%1.4f' % (epoch+1, n_epochs, train_nll, validation_nll))\n",
    "\n",
    "torch.save(net.state_dict(), save_dir + 'glow_model.pth')\n",
    "torch.save(optimizer.state_dict(), save_dir + 'glow_optimizer.pth')\n",
    "\n",
    "np.save(save_dir + 'train_nll_list.npy', np.array(train_nll_list))\n",
    "np.save(save_dir + 'validation_nll_list.npy', np.array(validation_nll_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
