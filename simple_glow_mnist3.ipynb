{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision verseion: 0.2.1\n",
      "Is GPU avaibale: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision verseion:', torchvision.__version__)\n",
    "print('Is GPU avaibale:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings (バッチサイズとデバイス)\n",
    "batchsize = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of training data: 60000\n",
      "the number of validation data: 10000\n"
     ]
    }
   ],
   "source": [
    "# データセットの準備\n",
    "# Tensorにしつつ、 (-1 ~ 1)の範囲に正規化\n",
    "\n",
    "def preprocess(tensor):\n",
    "    return tensor - 0.5\n",
    "\n",
    "tf = transforms.Compose([transforms.ToTensor(), preprocess])\n",
    "\n",
    "# データセットをロード(今回はMNIST)\n",
    "# 本当はtraining data, validation data, test dataに分けるべきだが、今回は簡便のため2つに分ける.\n",
    "mnist_train = datasets.MNIST(root = '../../data/MNIST',\n",
    "                                 train = True,\n",
    "                                 transform = tf,\n",
    "                                 download = False)\n",
    "mnist_validation = datasets.MNIST(root = '../../data/MNIST',\n",
    "                                      train = False,\n",
    "                                      transform = tf)\n",
    "\n",
    "# データローダーを作成\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size = batchsize, shuffle = True)\n",
    "mnist_validation_loader = DataLoader(mnist_validation, batch_size = batchsize, shuffle = False)\n",
    "\n",
    "print('the number of training data:', len(mnist_train))\n",
    "print('the number of validation data:', len(mnist_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actnormの実装\n",
    "class ActNorm2d(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ActNorm2d, self).__init__()\n",
    "        size = [1, num_features, 1, 1]\n",
    "        self.register_parameter('bias', nn.Parameter(torch.zeros(*size)))\n",
    "        self.register_parameter('log_s', nn.Parameter(torch.zeros(*size)))\n",
    "        self.inited = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.inited:\n",
    "            self.initialize_parameters(x)\n",
    "        \n",
    "        z = torch.exp(self.log_s) * x + self.bias\n",
    "        log_det_jacobian = self.calculate_log_det_jacobian(x)\n",
    "        return z, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        x = (z - self.bias) * torch.exp(-self.log_s)\n",
    "        return x\n",
    "\n",
    "    def calculate_log_det_jacobian(self, x):\n",
    "        h, w = x.size(2), x.size(3)\n",
    "        return h * w * torch.sum(self.log_s)\n",
    "    \n",
    "    def initialize_parameters(self, first_minibatch_x):\n",
    "        # cloneいるか？ .detachこれでいい？\n",
    "        bias = -1.0 * self.multidim_mean(first_minibatch_x.clone().detach(), dims=[0, 2, 3])\n",
    "        var_s = self.multidim_mean((first_minibatch_x.clone().detach() + bias) ** 2, dims=[0, 2, 3])\n",
    "        log_s = torch.log(1 / (torch.sqrt(var_s) + 1e-6))\n",
    "        \n",
    "        self.bias.data.copy_(bias.data)\n",
    "        self.log_s.data.copy_(log_s.data)\n",
    "        \n",
    "        self.inited = True\n",
    "            \n",
    "    def multidim_mean(self, tensor, dims):\n",
    "        dims = sorted(dims)\n",
    "        for d in dims:\n",
    "            tensor = tensor.mean(dim=d, keepdim=True)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invertible 1x1 convolutionの実装\n",
    "class Invertible1x1Conv2d(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Invertible1x1Conv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(num_features, num_features, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        W = torch.qr(torch.FloatTensor(num_features, num_features).normal_())[0]\n",
    "        \n",
    "        if torch.det(W) < 0:\n",
    "            W[:,0] = -W[:,0]\n",
    "\n",
    "        self.conv.weight.data = W.view(num_features, num_features, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.conv(x)\n",
    "        log_det_jacobian = self.calculate_log_det_jacobian(x)\n",
    "        return z, log_det_jacobian\n",
    "        \n",
    "    def inverse(self, z, train_finished=False):\n",
    "        if train_finished:\n",
    "            if not hasattr(self, 'W_inverse'):\n",
    "                W = self.conv.weight.squeeze()\n",
    "                W_inverse = W.inverse()\n",
    "                self.W_inverse = W_inverse.view(*W_inverse.size(), 1, 1)\n",
    "            x = F.conv2d(z, self.W_inverse, bias=None, stride=1, padding=0)\n",
    "        else:\n",
    "            W = self.conv.weight.squeeze()\n",
    "            W_inverse = W.inverse().view(*W.size(), 1, 1)\n",
    "            x = F.conv2d(z, W_inverse, bias=None, stride=1, padding=0)\n",
    "        return x\n",
    "        \n",
    "    def calculate_log_det_jacobian(self, x):\n",
    "        W = self.conv.weight.squeeze()\n",
    "        h, w = x.size(2), x.size(3)\n",
    "        return h * w * torch.logdet(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupling layerで使われるCNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, affine=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.affine = affine\n",
    "        if affine:\n",
    "            n_out = n_in*2\n",
    "        else:\n",
    "            n_out = n_in\n",
    "            \n",
    "        self.cv1 = nn.Conv2d(n_in, n_hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.ac1 = ActNorm2d(n_hidden)\n",
    "        self.cv2 = nn.Conv2d(n_hidden, n_hidden, kernel_size=1, stride=1, padding=0)\n",
    "        self.ac2 = ActNorm2d(n_hidden)\n",
    "        self.cv3 = nn.Conv2d(n_hidden, n_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, CNN_input):\n",
    "        out = F.relu(self.ac1(self.cv1(CNN_input))[0])\n",
    "        out = F.relu(self.ac2(self.cv2(out))[0])\n",
    "        if self.affine:\n",
    "            out = self.cv3(out)\n",
    "            n_half = int(out.size(1) / 2)\n",
    "            log_s = torch.tanh(out[:,:n_half,:,:])\n",
    "            bias = out[:,n_half:,:,:]\n",
    "            return [log_s, bias]\n",
    "        else:\n",
    "            bias = self.cv3(out)\n",
    "            return bias\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.cv1.weight.data.normal_(0, 0.05)\n",
    "        self.cv1.bias.data.zero_()\n",
    "        self.cv2.weight.data.normal_(0, 0.05)\n",
    "        self.cv2.bias.data.zero_()\n",
    "        self.cv3.weight.data.zero_()\n",
    "        self.cv3.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupling layerの実装\n",
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, num_features, n_hidden, affine=True):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "        \n",
    "        assert num_features % 2 == 0\n",
    "        self.n_half = int(num_features / 2)\n",
    "        self.affine = affine\n",
    "        \n",
    "        self.CNN = CNN(self.n_half, n_hidden, affine)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_a = x[:,:self.n_half,:,:]\n",
    "        x_b = x[:,self.n_half:,:,:]\n",
    "        \n",
    "        CNN_output = self.CNN(x_a)\n",
    "        \n",
    "        if self.affine:\n",
    "            log_s = CNN_output[0]\n",
    "            bias = CNN_output[1]\n",
    "            z_b = torch.exp(log_s) * x_b + bias\n",
    "            # z_b = (x_b + bias) * torch.exp(log_s)\n",
    "        else:\n",
    "            log_s = None\n",
    "            z_b = x_b + CNN_output\n",
    "            \n",
    "        z = torch.cat([x_a, z_b], dim=1)\n",
    "        log_det_jacobian = self.calculate_log_det_jacobian(log_s)\n",
    "        return z, log_det_jacobian\n",
    "        \n",
    "    def inverse(self, z):\n",
    "        z_a = z[:,:self.n_half,:,:]\n",
    "        z_b = z[:,self.n_half:,:,:]\n",
    "        \n",
    "        CNN_output = self.CNN(z_a)\n",
    "        \n",
    "        if self.affine:\n",
    "            log_s = CNN_output[0]\n",
    "            bias = CNN_output[1]\n",
    "            x_b = (z_b - bias) * torch.exp(-log_s)\n",
    "        else:\n",
    "            x_b = z_b - CNN_output\n",
    "            \n",
    "        x = torch.cat([z_a, x_b], dim=1)\n",
    "        return x\n",
    "        \n",
    "    def calculate_log_det_jacobian(self, log_s):\n",
    "        if self.affine:\n",
    "            return torch.sum(log_s)\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上３つをまとめたFlow\n",
    "class StepofFlow(nn.Module):\n",
    "    def __init__(self, num_features, n_hidden, affine=True):\n",
    "        super(StepofFlow, self).__init__()\n",
    "        self.actnorm = ActNorm2d(num_features)\n",
    "        self.invertible1x1conv = Invertible1x1Conv2d(num_features)\n",
    "        self.couplinglayer = CouplingLayer(num_features, n_hidden, affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, ldj_actnorm  = self.actnorm(x)\n",
    "        # print('act', ldj_actnorm)\n",
    "        x, ldj_1x1conv  = self.invertible1x1conv(x)\n",
    "        # print('1x1conv', ldj_1x1conv)\n",
    "        z, ldj_coupling = self.couplinglayer(x)\n",
    "        # print('coupling', ldj_coupling)\n",
    "        log_det_jacobian = ldj_actnorm + ldj_1x1conv + ldj_coupling\n",
    "        return z, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, z, train_finished=False):\n",
    "        z = self.couplinglayer.inverse(z)\n",
    "        z = self.invertible1x1conv.inverse(z, train_finished)\n",
    "        x = self.actnorm.inverse(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glow本体\n",
    "class Glow(nn.Module):\n",
    "    def __init__(self, L, K, num_input_features, n_hidden_list, affine=True):\n",
    "        super(Glow, self).__init__()\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        \n",
    "        num_features = num_input_features\n",
    "        assert len(n_hidden_list) == L*K\n",
    "        \n",
    "        self.flow = torch.nn.ModuleList()\n",
    "        for l in range(L):\n",
    "            # squeeze\n",
    "            num_features *= 4\n",
    "            for k in range(K):\n",
    "                # step of flow\n",
    "                self.flow.append(StepofFlow(num_features, int(n_hidden_list[l*K + k]), affine))\n",
    "            # split\n",
    "            num_features = num_features // 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = []\n",
    "        log_det_jacobian = 0\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            # squeeze\n",
    "            x = self.squeeze(x)\n",
    "            for k in range(self.K):\n",
    "                # step of flow\n",
    "                x, ldj = self.flow[l*self.K + k](x)\n",
    "                log_det_jacobian += ldj\n",
    "            # split\n",
    "            if l == self.L-1:\n",
    "                z.append(x.view(x.size(0), -1))\n",
    "            else:\n",
    "                z.append(x[:,:x.size(1)//2,:,:].view(x.size(0), -1))\n",
    "                x = x[:,x.size(1)//2:,:,:]\n",
    "        \n",
    "        z = torch.cat(z, dim=1)\n",
    "        if not hasattr(self, 'Z'):\n",
    "            batchsize, Z_dim = z.size()\n",
    "            self.Z = MultivariateNormal(torch.zeros(Z_dim).to(device), torch.eye(Z_dim).to(device))\n",
    "            self.last_z_shape = x.size()[1:]\n",
    "            \n",
    "        return z, log_det_jacobian\n",
    "        \n",
    "    def inverse(self, z, train_finished=False):\n",
    "        x_dim = self.last_z_shape[0] * self.last_z_shape[1] * self.last_z_shape[2]\n",
    "        for l in reversed(range(self.L)):\n",
    "            if l == self.L-1:\n",
    "                x = z[:,-x_dim:].view(-1, *self.last_z_shape)\n",
    "            else:\n",
    "                z_in = z[:,-x_dim*2:-x_dim]\n",
    "                x = torch.cat([z_in.view(*x.size()), x], dim = 1)\n",
    "                x_dim = x_dim*2\n",
    "                \n",
    "            for k in reversed(range(self.K)):\n",
    "                x = self.flow[l*self.K + k].inverse(x, train_finished)\n",
    "                \n",
    "            x = self.unsqueeze(x)\n",
    "        return x\n",
    "                \n",
    "    def squeeze(self, x, factor=2):\n",
    "        batchsize, channels, height, width = x.size()\n",
    "        assert height % factor == 0\n",
    "        assert width % factor == 0\n",
    "        z = x.view(batchsize, channels, height // factor, factor, width // factor, factor)\n",
    "        z = z.permute(0, 1, 3, 5, 2, 4)\n",
    "        z = z.contiguous().view(batchsize, channels * factor**2, height // factor, width // factor)\n",
    "        return z\n",
    "    \n",
    "    def unsqueeze(self, z, factor=2):\n",
    "        batchsize, channels, height, width = z.size()\n",
    "        x = z.view(batchsize, channels // (factor**2), factor, factor, height, width)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3)\n",
    "        x = x.contiguous().view(batchsize, channels // (factor**2), height * factor, width * factor)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 255712\n"
     ]
    }
   ],
   "source": [
    "net = Glow(L=2, K=16, num_input_features=1, n_hidden_list=np.ones(2*16)*64, affine=False)\n",
    "net = net.to(device)\n",
    "\n",
    "warm_up_epochs = 10\n",
    "learning_rate = 0.001 # * 10 ** (-warm_up_epochs)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "n_epochs = 100\n",
    "save_image_interval = 1\n",
    "n_save_image = 25\n",
    "save_dir = '../../data/glow_MNIST/'\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters:', num_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    for sample_x, _ in train_loader:\n",
    "        sample_x = sample_x.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict_z, log_det_jacobian = net(sample_x)\n",
    "        loss = -1 * (torch.mean(net.Z.log_prob(predict_z)) + log_det_jacobian) / predict_z.size(1) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(validation_loader, epoch):\n",
    "    net.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sample_x, _ in validation_loader:\n",
    "            sample_x = sample_x.to(device)\n",
    "            \n",
    "            predict_z, log_det_jacobian = net(sample_x)\n",
    "            loss = -1 * (torch.mean(net.Z.log_prob(predict_z)) + log_det_jacobian) / predict_z.size(1) \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if epoch % save_image_interval == 0:\n",
    "            sample_z = net.Z.sample((n_save_image,))\n",
    "            predict_x = net.inverse(sample_z)\n",
    "            print(torch.mean(predict_x))\n",
    "            save_image(predict_x.data.cpu(), '{}/epoch_{}.png'.format(save_dir, epoch), nrow=5, normalize=True)\n",
    "            \n",
    "    return running_loss / len(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3255, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 1/100] train_nll:-1.5443 validation_nll:-2.1556\n",
      "tensor(-0.3612, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 2/100] train_nll:-1.4756 validation_nll:-2.0275\n",
      "tensor(-9251.0625, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 3/100] train_nll:4.8424 validation_nll:0.2030\n",
      "tensor(-62.2197, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 4/100] train_nll:0.0395 validation_nll:-0.0833\n",
      "tensor(43.2316, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 5/100] train_nll:-0.1946 validation_nll:-0.3091\n",
      "tensor(1.9931, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 6/100] train_nll:-0.4262 validation_nll:-0.5451\n",
      "tensor(0.1284, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 7/100] train_nll:-0.6548 validation_nll:-0.7650\n",
      "tensor(-0.2133, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 8/100] train_nll:-0.8674 validation_nll:-0.9733\n",
      "tensor(-0.1834, device='cuda:0')\n",
      "0.001\n",
      "epoch[ 9/100] train_nll:-1.0500 validation_nll:-1.1330\n",
      "tensor(-0.2453, device='cuda:0')\n",
      "0.001\n",
      "epoch[10/100] train_nll:-1.2176 validation_nll:-1.3098\n",
      "tensor(1750157.5000, device='cuda:0')\n",
      "0.001\n",
      "epoch[11/100] train_nll:1152258.8981 validation_nll:1.9236\n",
      "tensor(807283.5625, device='cuda:0')\n",
      "0.001\n",
      "epoch[12/100] train_nll:1.5092 validation_nll:1.2662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-5c0333d12d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_validation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-b79984b44ccd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpredict_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det_jacobian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_det_jacobian\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpredict_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_nll_list = []\n",
    "validation_nll_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_nll = train(mnist_train_loader)\n",
    "    validation_nll = validation(mnist_validation_loader, epoch)\n",
    "    \n",
    "    train_nll_list.append(train_nll)\n",
    "    validation_nll_list.append(validation_nll)\n",
    "    \n",
    "    # if epoch < warm_up_epochs:\n",
    "    #    optimizer.param_groups[0]['lr'] *= 10\n",
    "    \n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    print('epoch[%2d/%2d] train_nll:%1.4f validation_nll:%1.4f' % (epoch+1, n_epochs, train_nll, validation_nll))\n",
    "\n",
    "torch.save(net.state_dict(), save_dir + 'glow_model.pth')\n",
    "torch.save(optimizer.state_dict(), save_dir + 'glow_optimizer.pth')\n",
    "\n",
    "np.save(save_dir + 'train_nll_list.npy', np.array(train_nll_list))\n",
    "np.save(save_dir + 'validation_nll_list.npy', np.array(validation_nll_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
